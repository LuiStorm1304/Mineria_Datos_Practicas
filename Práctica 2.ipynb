{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X1xALvaQWBX_"
   },
   "source": [
    "# Práctica 1: primera exploración a los datos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ciqUoT0yixVA"
   },
   "source": [
    "##### Cookbook [@data_mining_2020_1](https://nbviewer.jupyter.org/github/JacoboGGLeon/data_mining_2020_1/blob/master/README.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oc1R0TifiqXB"
   },
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZYWUvNR1yyLu"
   },
   "source": [
    "* [Syntactic n-grams in Computational Linguistics](https://www.springer.com/gp/book/9783030147709)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOljQUJIirjB"
   },
   "source": [
    "## Recipe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np    \n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'al',\n",
       " 'algo',\n",
       " 'algunas',\n",
       " 'algunos',\n",
       " 'ante',\n",
       " 'antes',\n",
       " 'como',\n",
       " 'con',\n",
       " 'contra',\n",
       " 'cual',\n",
       " 'cuando',\n",
       " 'de',\n",
       " 'del',\n",
       " 'desde',\n",
       " 'donde',\n",
       " 'durante',\n",
       " 'e',\n",
       " 'el',\n",
       " 'ella',\n",
       " 'ellas',\n",
       " 'ellos',\n",
       " 'en',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'erais',\n",
       " 'eran',\n",
       " 'eras',\n",
       " 'eres',\n",
       " 'es',\n",
       " 'esa',\n",
       " 'esas',\n",
       " 'ese',\n",
       " 'eso',\n",
       " 'esos',\n",
       " 'esta',\n",
       " 'estaba',\n",
       " 'estabais',\n",
       " 'estaban',\n",
       " 'estabas',\n",
       " 'estad',\n",
       " 'estada',\n",
       " 'estadas',\n",
       " 'estado',\n",
       " 'estados',\n",
       " 'estamos',\n",
       " 'estando',\n",
       " 'estar',\n",
       " 'estaremos',\n",
       " 'estará',\n",
       " 'estarán',\n",
       " 'estarás',\n",
       " 'estaré',\n",
       " 'estaréis',\n",
       " 'estaría',\n",
       " 'estaríais',\n",
       " 'estaríamos',\n",
       " 'estarían',\n",
       " 'estarías',\n",
       " 'estas',\n",
       " 'este',\n",
       " 'estemos',\n",
       " 'esto',\n",
       " 'estos',\n",
       " 'estoy',\n",
       " 'estuve',\n",
       " 'estuviera',\n",
       " 'estuvierais',\n",
       " 'estuvieran',\n",
       " 'estuvieras',\n",
       " 'estuvieron',\n",
       " 'estuviese',\n",
       " 'estuvieseis',\n",
       " 'estuviesen',\n",
       " 'estuvieses',\n",
       " 'estuvimos',\n",
       " 'estuviste',\n",
       " 'estuvisteis',\n",
       " 'estuviéramos',\n",
       " 'estuviésemos',\n",
       " 'estuvo',\n",
       " 'está',\n",
       " 'estábamos',\n",
       " 'estáis',\n",
       " 'están',\n",
       " 'estás',\n",
       " 'esté',\n",
       " 'estéis',\n",
       " 'estén',\n",
       " 'estés',\n",
       " 'fue',\n",
       " 'fuera',\n",
       " 'fuerais',\n",
       " 'fueran',\n",
       " 'fueras',\n",
       " 'fueron',\n",
       " 'fuese',\n",
       " 'fueseis',\n",
       " 'fuesen',\n",
       " 'fueses',\n",
       " 'fui',\n",
       " 'fuimos',\n",
       " 'fuiste',\n",
       " 'fuisteis',\n",
       " 'fuéramos',\n",
       " 'fuésemos',\n",
       " 'ha',\n",
       " 'habida',\n",
       " 'habidas',\n",
       " 'habido',\n",
       " 'habidos',\n",
       " 'habiendo',\n",
       " 'habremos',\n",
       " 'habrá',\n",
       " 'habrán',\n",
       " 'habrás',\n",
       " 'habré',\n",
       " 'habréis',\n",
       " 'habría',\n",
       " 'habríais',\n",
       " 'habríamos',\n",
       " 'habrían',\n",
       " 'habrías',\n",
       " 'habéis',\n",
       " 'había',\n",
       " 'habíais',\n",
       " 'habíamos',\n",
       " 'habían',\n",
       " 'habías',\n",
       " 'han',\n",
       " 'has',\n",
       " 'hasta',\n",
       " 'hay',\n",
       " 'haya',\n",
       " 'hayamos',\n",
       " 'hayan',\n",
       " 'hayas',\n",
       " 'hayáis',\n",
       " 'he',\n",
       " 'hemos',\n",
       " 'hube',\n",
       " 'hubiera',\n",
       " 'hubierais',\n",
       " 'hubieran',\n",
       " 'hubieras',\n",
       " 'hubieron',\n",
       " 'hubiese',\n",
       " 'hubieseis',\n",
       " 'hubiesen',\n",
       " 'hubieses',\n",
       " 'hubimos',\n",
       " 'hubiste',\n",
       " 'hubisteis',\n",
       " 'hubiéramos',\n",
       " 'hubiésemos',\n",
       " 'hubo',\n",
       " 'la',\n",
       " 'las',\n",
       " 'le',\n",
       " 'les',\n",
       " 'lo',\n",
       " 'los',\n",
       " 'me',\n",
       " 'mi',\n",
       " 'mis',\n",
       " 'mucho',\n",
       " 'muchos',\n",
       " 'muy',\n",
       " 'más',\n",
       " 'mí',\n",
       " 'mía',\n",
       " 'mías',\n",
       " 'mío',\n",
       " 'míos',\n",
       " 'nada',\n",
       " 'ni',\n",
       " 'no',\n",
       " 'nos',\n",
       " 'nosotras',\n",
       " 'nosotros',\n",
       " 'nuestra',\n",
       " 'nuestras',\n",
       " 'nuestro',\n",
       " 'nuestros',\n",
       " 'o',\n",
       " 'os',\n",
       " 'otra',\n",
       " 'otras',\n",
       " 'otro',\n",
       " 'otros',\n",
       " 'para',\n",
       " 'pero',\n",
       " 'poco',\n",
       " 'por',\n",
       " 'porque',\n",
       " 'que',\n",
       " 'quien',\n",
       " 'quienes',\n",
       " 'qué',\n",
       " 'se',\n",
       " 'sea',\n",
       " 'seamos',\n",
       " 'sean',\n",
       " 'seas',\n",
       " 'sentid',\n",
       " 'sentida',\n",
       " 'sentidas',\n",
       " 'sentido',\n",
       " 'sentidos',\n",
       " 'seremos',\n",
       " 'será',\n",
       " 'serán',\n",
       " 'serás',\n",
       " 'seré',\n",
       " 'seréis',\n",
       " 'sería',\n",
       " 'seríais',\n",
       " 'seríamos',\n",
       " 'serían',\n",
       " 'serías',\n",
       " 'seáis',\n",
       " 'siente',\n",
       " 'sin',\n",
       " 'sintiendo',\n",
       " 'sobre',\n",
       " 'sois',\n",
       " 'somos',\n",
       " 'son',\n",
       " 'soy',\n",
       " 'su',\n",
       " 'sus',\n",
       " 'suya',\n",
       " 'suyas',\n",
       " 'suyo',\n",
       " 'suyos',\n",
       " 'sí',\n",
       " 'también',\n",
       " 'tanto',\n",
       " 'te',\n",
       " 'tendremos',\n",
       " 'tendrá',\n",
       " 'tendrán',\n",
       " 'tendrás',\n",
       " 'tendré',\n",
       " 'tendréis',\n",
       " 'tendría',\n",
       " 'tendríais',\n",
       " 'tendríamos',\n",
       " 'tendrían',\n",
       " 'tendrías',\n",
       " 'tened',\n",
       " 'tenemos',\n",
       " 'tenga',\n",
       " 'tengamos',\n",
       " 'tengan',\n",
       " 'tengas',\n",
       " 'tengo',\n",
       " 'tengáis',\n",
       " 'tenida',\n",
       " 'tenidas',\n",
       " 'tenido',\n",
       " 'tenidos',\n",
       " 'teniendo',\n",
       " 'tenéis',\n",
       " 'tenía',\n",
       " 'teníais',\n",
       " 'teníamos',\n",
       " 'tenían',\n",
       " 'tenías',\n",
       " 'ti',\n",
       " 'tiene',\n",
       " 'tienen',\n",
       " 'tienes',\n",
       " 'todo',\n",
       " 'todos',\n",
       " 'tu',\n",
       " 'tus',\n",
       " 'tuve',\n",
       " 'tuviera',\n",
       " 'tuvierais',\n",
       " 'tuvieran',\n",
       " 'tuvieras',\n",
       " 'tuvieron',\n",
       " 'tuviese',\n",
       " 'tuvieseis',\n",
       " 'tuviesen',\n",
       " 'tuvieses',\n",
       " 'tuvimos',\n",
       " 'tuviste',\n",
       " 'tuvisteis',\n",
       " 'tuviéramos',\n",
       " 'tuviésemos',\n",
       " 'tuvo',\n",
       " 'tuya',\n",
       " 'tuyas',\n",
       " 'tuyo',\n",
       " 'tuyos',\n",
       " 'tú',\n",
       " 'un',\n",
       " 'una',\n",
       " 'uno',\n",
       " 'unos',\n",
       " 'vosotras',\n",
       " 'vosotros',\n",
       " 'vuestra',\n",
       " 'vuestras',\n",
       " 'vuestro',\n",
       " 'vuestros',\n",
       " 'y',\n",
       " 'ya',\n",
       " 'yo',\n",
       " 'él',\n",
       " 'éramos'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimento 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preface Automated Machine Learning Methods, Systems, Challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Párrafo 1\n",
    "text_01 = \"I have been very passionate about automating machine learning myself ever since our Automatic Statistician project started back in 2014. I want us to be really ambitious in this endeavor; we should try to automate all aspects of the entire machine learning and data analysis pipeline. This includes automating data collection and experiment design; automating data cleanup and missing data imputa- tion; automating feature selection and transformation; automating model discovery, criticism, and explanation; automating the allocation of computational resources; automating hyperparameter optimization; automating inference; and automating model monitoring and anomaly detection. This is a huge list of things, and we’d optimally like to automate all of it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Párrafo 2\n",
    "text_02 = \"There is a caveat of course. While full automation can motivate scientific research and provide a long-term engineering goal, in practice, we probably want to semiautomate most of these and gradually remove the human in the loop as needed. Along the way, what is going to happen if we try to do all this automation is that we are likely to develop powerful tools that will help make the practice of machine learning, first of all, more systematic (since it’s very ad hoc these days) and also more efficient.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Párrafo 3\n",
    "text_03 = \"These are worthy goals even if we did not succeed in the final goal of automation, but as this book demonstrates, current AutoML methods can already surpass human machine learning experts in several tasks. This trend is likely only going to intensify as we’re making progress and as computation becomes ever cheaper, and AutoML is therefore clearly one of the topics that is here to stay. It is a great time to get involved in AutoML, and this book is an excellent starting point.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimento 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://twitter.com/GaeboraKae/status/1169424976413372416\n",
    "text_01 = \"Quisiera entender que le ven de rico o divertido a hacer cebo en el metro, de verdad, acabo de ver a cuatro parejas en la misma área dandolo todo con su queso\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://twitter.com/VICEenEspanol/status/1169705966234943488\n",
    "text_02 = \"Dos mujeres que se tomaban una selfie al lado de una pista de aterrizaje serrana murieron, al parecer, accidentalmente; un piloto aviador que solía viajar a la Sierra Tarahumara y un maestro de artes marciales retirado fueron asesinados en 2017.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://twitter.com/Anaro74/status/1169579828963463168\n",
    "text_03 = \"Buenos días desde un país donde el Subsecretario de Educación dice que el comunismo es necesario para transformar a México; donde se disparan 312% los casos de dengue porque no compraron insecticidas, Mireles llama pirujas a las concubinas y el Presi quiere quitar el INE Café?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'have',\n",
       " 'been',\n",
       " 'very',\n",
       " 'passionate',\n",
       " 'about',\n",
       " 'automating',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'myself',\n",
       " 'ever',\n",
       " 'since',\n",
       " 'our',\n",
       " 'automatic',\n",
       " 'statistician',\n",
       " 'project',\n",
       " 'started',\n",
       " 'back',\n",
       " 'in',\n",
       " '2014',\n",
       " 'i',\n",
       " 'want',\n",
       " 'us',\n",
       " 'to',\n",
       " 'be',\n",
       " 'really',\n",
       " 'ambitious',\n",
       " 'in',\n",
       " 'this',\n",
       " 'endeavor',\n",
       " 'we',\n",
       " 'should',\n",
       " 'try',\n",
       " 'to',\n",
       " 'automate',\n",
       " 'all',\n",
       " 'aspects',\n",
       " 'of',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'and',\n",
       " 'data',\n",
       " 'analysis',\n",
       " 'pipeline',\n",
       " 'this',\n",
       " 'includes',\n",
       " 'automating',\n",
       " 'data',\n",
       " 'collection',\n",
       " 'and',\n",
       " 'experiment',\n",
       " 'design',\n",
       " 'automating',\n",
       " 'data',\n",
       " 'cleanup',\n",
       " 'and',\n",
       " 'missing',\n",
       " 'data',\n",
       " 'imputa',\n",
       " 'tion',\n",
       " 'automating',\n",
       " 'feature',\n",
       " 'selection',\n",
       " 'and',\n",
       " 'transformation',\n",
       " 'automating',\n",
       " 'model',\n",
       " 'discovery',\n",
       " 'criticism',\n",
       " 'and',\n",
       " 'explanation',\n",
       " 'automating',\n",
       " 'the',\n",
       " 'allocation',\n",
       " 'of',\n",
       " 'computational',\n",
       " 'resources',\n",
       " 'automating',\n",
       " 'hyperparameter',\n",
       " 'optimization',\n",
       " 'automating',\n",
       " 'inference',\n",
       " 'and',\n",
       " 'automating',\n",
       " 'model',\n",
       " 'monitoring',\n",
       " 'and',\n",
       " 'anomaly',\n",
       " 'detection',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'huge',\n",
       " 'list',\n",
       " 'of',\n",
       " 'things',\n",
       " 'and',\n",
       " 'we',\n",
       " 'd',\n",
       " 'optimally',\n",
       " 'like',\n",
       " 'to',\n",
       " 'automate',\n",
       " 'all',\n",
       " 'of',\n",
       " 'it']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_01_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i have been very passionate about automating machine learning myself ever since our automatic statistician project started back in 2014. i want us to be really ambitious in this endeavor; we should try to automate all aspects of the entire machine learning and data analysis pipeline. this includes automating data collection and experiment design; automating data cleanup and missing data imputa- tion; automating feature selection and transformation; automating model discovery, criticism, and explanation; automating the allocation of computational resources; automating hyperparameter optimization; automating inference; and automating model monitoring and anomaly detection. this is a huge list of things, and we’d optimally like to automate all of it.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_01.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['passionate', 'automating', 'machine', 'learning', 'ever', 'since', 'automatic', 'statistician', 'project', 'started', 'back', '2014', 'want', 'us', 'really', 'ambitious', 'endeavor', 'try', 'automate', 'aspects', 'entire', 'machine', 'learning', 'data', 'analysis', 'pipeline', 'includes', 'automating', 'data', 'collection', 'experiment', 'design', 'automating', 'data', 'cleanup', 'missing', 'data', 'imputa', 'tion', 'automating', 'feature', 'selection', 'transformation', 'automating', 'model', 'discovery', 'criticism', 'explanation', 'automating', 'allocation', 'computational', 'resources', 'automating', 'hyperparameter', 'optimization', 'automating', 'inference', 'automating', 'model', 'monitoring', 'anomaly', 'detection', 'huge', 'list', 'things', 'optimally', 'like', 'automate']\n"
     ]
    }
   ],
   "source": [
    "text_01_tokens = tokenizer.tokenize(text_01.lower()) #tokenizar y quitar signos de puntuación\n",
    "#print(text_01_tokens)\n",
    "\n",
    "text_01_tokens_wout_stopwords = []\n",
    "\n",
    "for word in text_01_tokens:\n",
    "    if word not in stopwords.words('english'): text_01_tokens_wout_stopwords.append(word)\n",
    "\n",
    "print(text_01_tokens_wout_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['caveat', 'course', 'full', 'automation', 'motivate', 'scientific', 'research', 'provide', 'long', 'term', 'engineering', 'goal', 'practice', 'probably', 'want', 'semiautomate', 'gradually', 'remove', 'human', 'loop', 'needed', 'along', 'way', 'going', 'happen', 'try', 'automation', 'likely', 'develop', 'powerful', 'tools', 'help', 'make', 'practice', 'machine', 'learning', 'first', 'systematic', 'since', 'ad', 'hoc', 'days', 'also', 'efficient']\n"
     ]
    }
   ],
   "source": [
    "text_02_tokens = tokenizer.tokenize(text_02.lower()) \n",
    "#print(text_02_tokens)\n",
    "\n",
    "text_02_tokens_wout_stopwords = []\n",
    "\n",
    "for word in text_02_tokens:\n",
    "    if word not in stopwords.words('english'): text_02_tokens_wout_stopwords.append(word)\n",
    "\n",
    "print(text_02_tokens_wout_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['worthy', 'goals', 'even', 'succeed', 'final', 'goal', 'automation', 'book', 'demonstrates', 'current', 'automl', 'methods', 'already', 'surpass', 'human', 'machine', 'learning', 'experts', 'several', 'tasks', 'trend', 'likely', 'going', 'intensify', 'making', 'progress', 'computation', 'becomes', 'ever', 'cheaper', 'automl', 'therefore', 'clearly', 'one', 'topics', 'stay', 'great', 'time', 'get', 'involved', 'automl', 'book', 'excellent', 'starting', 'point']\n"
     ]
    }
   ],
   "source": [
    "text_03_tokens = tokenizer.tokenize(text_03.lower()) \n",
    "#print(text_03_tokens)\n",
    "\n",
    "text_03_tokens_wout_stopwords = []\n",
    "\n",
    "for word in text_03_tokens:\n",
    "    if word not in stopwords.words('english'): text_03_tokens_wout_stopwords.append(word)\n",
    "\n",
    "print(text_03_tokens_wout_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "44\n",
      "45\n",
      "180\n"
     ]
    }
   ],
   "source": [
    "print(len(text_01_tokens_wout_stopwords))\n",
    "print(len(text_02_tokens_wout_stopwords))\n",
    "print(len(text_03_tokens_wout_stopwords))\n",
    "print(len(text_01_tokens_wout_stopwords) + len(text_02_tokens_wout_stopwords) + len(text_01_tokens_wout_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de la BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_01': ['passionate',\n",
       "  'automating',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'ever',\n",
       "  'since',\n",
       "  'automatic',\n",
       "  'statistician',\n",
       "  'project',\n",
       "  'started',\n",
       "  'back',\n",
       "  '2014',\n",
       "  'want',\n",
       "  'us',\n",
       "  'really',\n",
       "  'ambitious',\n",
       "  'endeavor',\n",
       "  'try',\n",
       "  'automate',\n",
       "  'aspects',\n",
       "  'entire',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'data',\n",
       "  'analysis',\n",
       "  'pipeline',\n",
       "  'includes',\n",
       "  'automating',\n",
       "  'data',\n",
       "  'collection',\n",
       "  'experiment',\n",
       "  'design',\n",
       "  'automating',\n",
       "  'data',\n",
       "  'cleanup',\n",
       "  'missing',\n",
       "  'data',\n",
       "  'imputa',\n",
       "  'tion',\n",
       "  'automating',\n",
       "  'feature',\n",
       "  'selection',\n",
       "  'transformation',\n",
       "  'automating',\n",
       "  'model',\n",
       "  'discovery',\n",
       "  'criticism',\n",
       "  'explanation',\n",
       "  'automating',\n",
       "  'allocation',\n",
       "  'computational',\n",
       "  'resources',\n",
       "  'automating',\n",
       "  'hyperparameter',\n",
       "  'optimization',\n",
       "  'automating',\n",
       "  'inference',\n",
       "  'automating',\n",
       "  'model',\n",
       "  'monitoring',\n",
       "  'anomaly',\n",
       "  'detection',\n",
       "  'huge',\n",
       "  'list',\n",
       "  'things',\n",
       "  'optimally',\n",
       "  'like',\n",
       "  'automate'],\n",
       " 'text_02': ['caveat',\n",
       "  'course',\n",
       "  'full',\n",
       "  'automation',\n",
       "  'motivate',\n",
       "  'scientific',\n",
       "  'research',\n",
       "  'provide',\n",
       "  'long',\n",
       "  'term',\n",
       "  'engineering',\n",
       "  'goal',\n",
       "  'practice',\n",
       "  'probably',\n",
       "  'want',\n",
       "  'semiautomate',\n",
       "  'gradually',\n",
       "  'remove',\n",
       "  'human',\n",
       "  'loop',\n",
       "  'needed',\n",
       "  'along',\n",
       "  'way',\n",
       "  'going',\n",
       "  'happen',\n",
       "  'try',\n",
       "  'automation',\n",
       "  'likely',\n",
       "  'develop',\n",
       "  'powerful',\n",
       "  'tools',\n",
       "  'help',\n",
       "  'make',\n",
       "  'practice',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'first',\n",
       "  'systematic',\n",
       "  'since',\n",
       "  'ad',\n",
       "  'hoc',\n",
       "  'days',\n",
       "  'also',\n",
       "  'efficient'],\n",
       " 'text_03': ['worthy',\n",
       "  'goals',\n",
       "  'even',\n",
       "  'succeed',\n",
       "  'final',\n",
       "  'goal',\n",
       "  'automation',\n",
       "  'book',\n",
       "  'demonstrates',\n",
       "  'current',\n",
       "  'automl',\n",
       "  'methods',\n",
       "  'already',\n",
       "  'surpass',\n",
       "  'human',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'experts',\n",
       "  'several',\n",
       "  'tasks',\n",
       "  'trend',\n",
       "  'likely',\n",
       "  'going',\n",
       "  'intensify',\n",
       "  'making',\n",
       "  'progress',\n",
       "  'computation',\n",
       "  'becomes',\n",
       "  'ever',\n",
       "  'cheaper',\n",
       "  'automl',\n",
       "  'therefore',\n",
       "  'clearly',\n",
       "  'one',\n",
       "  'topics',\n",
       "  'stay',\n",
       "  'great',\n",
       "  'time',\n",
       "  'get',\n",
       "  'involved',\n",
       "  'automl',\n",
       "  'book',\n",
       "  'excellent',\n",
       "  'starting',\n",
       "  'point']}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicc_texts = {\"text_01\": text_01_tokens_wout_stopwords, \n",
    " \"text_02\": text_02_tokens_wout_stopwords, \n",
    " \"text_03\": text_03_tokens_wout_stopwords}\n",
    "dicc_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'passionate': 1,\n",
       " 'automating': 9,\n",
       " 'machine': 4,\n",
       " 'learning': 4,\n",
       " 'ever': 2,\n",
       " 'since': 2,\n",
       " 'automatic': 1,\n",
       " 'statistician': 1,\n",
       " 'project': 1,\n",
       " 'started': 1,\n",
       " 'back': 1,\n",
       " '2014': 1,\n",
       " 'want': 2,\n",
       " 'us': 1,\n",
       " 'really': 1,\n",
       " 'ambitious': 1,\n",
       " 'endeavor': 1,\n",
       " 'try': 2,\n",
       " 'automate': 2,\n",
       " 'aspects': 1,\n",
       " 'entire': 1,\n",
       " 'data': 4,\n",
       " 'analysis': 1,\n",
       " 'pipeline': 1,\n",
       " 'includes': 1,\n",
       " 'collection': 1,\n",
       " 'experiment': 1,\n",
       " 'design': 1,\n",
       " 'cleanup': 1,\n",
       " 'missing': 1,\n",
       " 'imputa': 1,\n",
       " 'tion': 1,\n",
       " 'feature': 1,\n",
       " 'selection': 1,\n",
       " 'transformation': 1,\n",
       " 'model': 2,\n",
       " 'discovery': 1,\n",
       " 'criticism': 1,\n",
       " 'explanation': 1,\n",
       " 'allocation': 1,\n",
       " 'computational': 1,\n",
       " 'resources': 1,\n",
       " 'hyperparameter': 1,\n",
       " 'optimization': 1,\n",
       " 'inference': 1,\n",
       " 'monitoring': 1,\n",
       " 'anomaly': 1,\n",
       " 'detection': 1,\n",
       " 'huge': 1,\n",
       " 'list': 1,\n",
       " 'things': 1,\n",
       " 'optimally': 1,\n",
       " 'like': 1,\n",
       " 'caveat': 1,\n",
       " 'course': 1,\n",
       " 'full': 1,\n",
       " 'automation': 3,\n",
       " 'motivate': 1,\n",
       " 'scientific': 1,\n",
       " 'research': 1,\n",
       " 'provide': 1,\n",
       " 'long': 1,\n",
       " 'term': 1,\n",
       " 'engineering': 1,\n",
       " 'goal': 2,\n",
       " 'practice': 2,\n",
       " 'probably': 1,\n",
       " 'semiautomate': 1,\n",
       " 'gradually': 1,\n",
       " 'remove': 1,\n",
       " 'human': 2,\n",
       " 'loop': 1,\n",
       " 'needed': 1,\n",
       " 'along': 1,\n",
       " 'way': 1,\n",
       " 'going': 2,\n",
       " 'happen': 1,\n",
       " 'likely': 2,\n",
       " 'develop': 1,\n",
       " 'powerful': 1,\n",
       " 'tools': 1,\n",
       " 'help': 1,\n",
       " 'make': 1,\n",
       " 'first': 1,\n",
       " 'systematic': 1,\n",
       " 'ad': 1,\n",
       " 'hoc': 1,\n",
       " 'days': 1,\n",
       " 'also': 1,\n",
       " 'efficient': 1,\n",
       " 'worthy': 1,\n",
       " 'goals': 1,\n",
       " 'even': 1,\n",
       " 'succeed': 1,\n",
       " 'final': 1,\n",
       " 'book': 2,\n",
       " 'demonstrates': 1,\n",
       " 'current': 1,\n",
       " 'automl': 3,\n",
       " 'methods': 1,\n",
       " 'already': 1,\n",
       " 'surpass': 1,\n",
       " 'experts': 1,\n",
       " 'several': 1,\n",
       " 'tasks': 1,\n",
       " 'trend': 1,\n",
       " 'intensify': 1,\n",
       " 'making': 1,\n",
       " 'progress': 1,\n",
       " 'computation': 1,\n",
       " 'becomes': 1,\n",
       " 'cheaper': 1,\n",
       " 'therefore': 1,\n",
       " 'clearly': 1,\n",
       " 'one': 1,\n",
       " 'topics': 1,\n",
       " 'stay': 1,\n",
       " 'great': 1,\n",
       " 'time': 1,\n",
       " 'get': 1,\n",
       " 'involved': 1,\n",
       " 'excellent': 1,\n",
       " 'starting': 1,\n",
       " 'point': 1}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicc_termns = {}\n",
    "\n",
    "for text in dicc_texts:\n",
    "    for word in dicc_texts[text]:\n",
    "        \n",
    "#        print(\"EVALUAR:\", word, \"EN\", text)\n",
    "        \n",
    "        if(word in dicc_termns):#incrementar palabras al diccionario\n",
    "            dicc_termns[word] = dicc_termns[word] + 1\n",
    "            \n",
    "#            print(word, \"IN\", \"dicc_termns\")\n",
    "            \n",
    "        elif(word not in dicc_termns):#agregar palabras al diccionario        \n",
    "            dicc_termns[word] = 1\n",
    "            \n",
    "#            print(word, \"NOT IN\", \"dicc_termns\")            \n",
    "\n",
    "print(len(dicc_termns))\n",
    "dicc_termns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matriz Término Documento (binaria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.zeros((len(dicc_texts), len(dicc_termns))) # Pre-allocate matrix\n",
    "#matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passionate IN text_01\n",
      "se agregó:  1.0 en:  0 0\n",
      "passionate NOT IN text_02\n",
      "se agregó:  0.0 en:  1 0\n",
      "passionate NOT IN text_03\n",
      "se agregó:  0.0 en:  2 0\n",
      "automating IN text_01\n",
      "se agregó:  1.0 en:  0 1\n",
      "automating NOT IN text_02\n",
      "se agregó:  0.0 en:  1 1\n",
      "automating NOT IN text_03\n",
      "se agregó:  0.0 en:  2 1\n",
      "machine IN text_01\n",
      "se agregó:  1.0 en:  0 2\n",
      "machine IN text_02\n",
      "se agregó:  1.0 en:  1 2\n",
      "machine IN text_03\n",
      "se agregó:  1.0 en:  2 2\n",
      "learning IN text_01\n",
      "se agregó:  1.0 en:  0 3\n",
      "learning IN text_02\n",
      "se agregó:  1.0 en:  1 3\n",
      "learning IN text_03\n",
      "se agregó:  1.0 en:  2 3\n",
      "ever IN text_01\n",
      "se agregó:  1.0 en:  0 4\n",
      "ever NOT IN text_02\n",
      "se agregó:  0.0 en:  1 4\n",
      "ever IN text_03\n",
      "se agregó:  1.0 en:  2 4\n",
      "since IN text_01\n",
      "se agregó:  1.0 en:  0 5\n",
      "since IN text_02\n",
      "se agregó:  1.0 en:  1 5\n",
      "since NOT IN text_03\n",
      "se agregó:  0.0 en:  2 5\n",
      "automatic IN text_01\n",
      "se agregó:  1.0 en:  0 6\n",
      "automatic NOT IN text_02\n",
      "se agregó:  0.0 en:  1 6\n",
      "automatic NOT IN text_03\n",
      "se agregó:  0.0 en:  2 6\n",
      "statistician IN text_01\n",
      "se agregó:  1.0 en:  0 7\n",
      "statistician NOT IN text_02\n",
      "se agregó:  0.0 en:  1 7\n",
      "statistician NOT IN text_03\n",
      "se agregó:  0.0 en:  2 7\n",
      "project IN text_01\n",
      "se agregó:  1.0 en:  0 8\n",
      "project NOT IN text_02\n",
      "se agregó:  0.0 en:  1 8\n",
      "project NOT IN text_03\n",
      "se agregó:  0.0 en:  2 8\n",
      "started IN text_01\n",
      "se agregó:  1.0 en:  0 9\n",
      "started NOT IN text_02\n",
      "se agregó:  0.0 en:  1 9\n",
      "started NOT IN text_03\n",
      "se agregó:  0.0 en:  2 9\n",
      "back IN text_01\n",
      "se agregó:  1.0 en:  0 10\n",
      "back NOT IN text_02\n",
      "se agregó:  0.0 en:  1 10\n",
      "back NOT IN text_03\n",
      "se agregó:  0.0 en:  2 10\n",
      "2014 IN text_01\n",
      "se agregó:  1.0 en:  0 11\n",
      "2014 NOT IN text_02\n",
      "se agregó:  0.0 en:  1 11\n",
      "2014 NOT IN text_03\n",
      "se agregó:  0.0 en:  2 11\n",
      "want IN text_01\n",
      "se agregó:  1.0 en:  0 12\n",
      "want IN text_02\n",
      "se agregó:  1.0 en:  1 12\n",
      "want NOT IN text_03\n",
      "se agregó:  0.0 en:  2 12\n",
      "us IN text_01\n",
      "se agregó:  1.0 en:  0 13\n",
      "us NOT IN text_02\n",
      "se agregó:  0.0 en:  1 13\n",
      "us NOT IN text_03\n",
      "se agregó:  0.0 en:  2 13\n",
      "really IN text_01\n",
      "se agregó:  1.0 en:  0 14\n",
      "really NOT IN text_02\n",
      "se agregó:  0.0 en:  1 14\n",
      "really NOT IN text_03\n",
      "se agregó:  0.0 en:  2 14\n",
      "ambitious IN text_01\n",
      "se agregó:  1.0 en:  0 15\n",
      "ambitious NOT IN text_02\n",
      "se agregó:  0.0 en:  1 15\n",
      "ambitious NOT IN text_03\n",
      "se agregó:  0.0 en:  2 15\n",
      "endeavor IN text_01\n",
      "se agregó:  1.0 en:  0 16\n",
      "endeavor NOT IN text_02\n",
      "se agregó:  0.0 en:  1 16\n",
      "endeavor NOT IN text_03\n",
      "se agregó:  0.0 en:  2 16\n",
      "try IN text_01\n",
      "se agregó:  1.0 en:  0 17\n",
      "try IN text_02\n",
      "se agregó:  1.0 en:  1 17\n",
      "try NOT IN text_03\n",
      "se agregó:  0.0 en:  2 17\n",
      "automate IN text_01\n",
      "se agregó:  1.0 en:  0 18\n",
      "automate NOT IN text_02\n",
      "se agregó:  0.0 en:  1 18\n",
      "automate NOT IN text_03\n",
      "se agregó:  0.0 en:  2 18\n",
      "aspects IN text_01\n",
      "se agregó:  1.0 en:  0 19\n",
      "aspects NOT IN text_02\n",
      "se agregó:  0.0 en:  1 19\n",
      "aspects NOT IN text_03\n",
      "se agregó:  0.0 en:  2 19\n",
      "entire IN text_01\n",
      "se agregó:  1.0 en:  0 20\n",
      "entire NOT IN text_02\n",
      "se agregó:  0.0 en:  1 20\n",
      "entire NOT IN text_03\n",
      "se agregó:  0.0 en:  2 20\n",
      "data IN text_01\n",
      "se agregó:  1.0 en:  0 21\n",
      "data NOT IN text_02\n",
      "se agregó:  0.0 en:  1 21\n",
      "data NOT IN text_03\n",
      "se agregó:  0.0 en:  2 21\n",
      "analysis IN text_01\n",
      "se agregó:  1.0 en:  0 22\n",
      "analysis NOT IN text_02\n",
      "se agregó:  0.0 en:  1 22\n",
      "analysis NOT IN text_03\n",
      "se agregó:  0.0 en:  2 22\n",
      "pipeline IN text_01\n",
      "se agregó:  1.0 en:  0 23\n",
      "pipeline NOT IN text_02\n",
      "se agregó:  0.0 en:  1 23\n",
      "pipeline NOT IN text_03\n",
      "se agregó:  0.0 en:  2 23\n",
      "includes IN text_01\n",
      "se agregó:  1.0 en:  0 24\n",
      "includes NOT IN text_02\n",
      "se agregó:  0.0 en:  1 24\n",
      "includes NOT IN text_03\n",
      "se agregó:  0.0 en:  2 24\n",
      "collection IN text_01\n",
      "se agregó:  1.0 en:  0 25\n",
      "collection NOT IN text_02\n",
      "se agregó:  0.0 en:  1 25\n",
      "collection NOT IN text_03\n",
      "se agregó:  0.0 en:  2 25\n",
      "experiment IN text_01\n",
      "se agregó:  1.0 en:  0 26\n",
      "experiment NOT IN text_02\n",
      "se agregó:  0.0 en:  1 26\n",
      "experiment NOT IN text_03\n",
      "se agregó:  0.0 en:  2 26\n",
      "design IN text_01\n",
      "se agregó:  1.0 en:  0 27\n",
      "design NOT IN text_02\n",
      "se agregó:  0.0 en:  1 27\n",
      "design NOT IN text_03\n",
      "se agregó:  0.0 en:  2 27\n",
      "cleanup IN text_01\n",
      "se agregó:  1.0 en:  0 28\n",
      "cleanup NOT IN text_02\n",
      "se agregó:  0.0 en:  1 28\n",
      "cleanup NOT IN text_03\n",
      "se agregó:  0.0 en:  2 28\n",
      "missing IN text_01\n",
      "se agregó:  1.0 en:  0 29\n",
      "missing NOT IN text_02\n",
      "se agregó:  0.0 en:  1 29\n",
      "missing NOT IN text_03\n",
      "se agregó:  0.0 en:  2 29\n",
      "imputa IN text_01\n",
      "se agregó:  1.0 en:  0 30\n",
      "imputa NOT IN text_02\n",
      "se agregó:  0.0 en:  1 30\n",
      "imputa NOT IN text_03\n",
      "se agregó:  0.0 en:  2 30\n",
      "tion IN text_01\n",
      "se agregó:  1.0 en:  0 31\n",
      "tion NOT IN text_02\n",
      "se agregó:  0.0 en:  1 31\n",
      "tion NOT IN text_03\n",
      "se agregó:  0.0 en:  2 31\n",
      "feature IN text_01\n",
      "se agregó:  1.0 en:  0 32\n",
      "feature NOT IN text_02\n",
      "se agregó:  0.0 en:  1 32\n",
      "feature NOT IN text_03\n",
      "se agregó:  0.0 en:  2 32\n",
      "selection IN text_01\n",
      "se agregó:  1.0 en:  0 33\n",
      "selection NOT IN text_02\n",
      "se agregó:  0.0 en:  1 33\n",
      "selection NOT IN text_03\n",
      "se agregó:  0.0 en:  2 33\n",
      "transformation IN text_01\n",
      "se agregó:  1.0 en:  0 34\n",
      "transformation NOT IN text_02\n",
      "se agregó:  0.0 en:  1 34\n",
      "transformation NOT IN text_03\n",
      "se agregó:  0.0 en:  2 34\n",
      "model IN text_01\n",
      "se agregó:  1.0 en:  0 35\n",
      "model NOT IN text_02\n",
      "se agregó:  0.0 en:  1 35\n",
      "model NOT IN text_03\n",
      "se agregó:  0.0 en:  2 35\n",
      "discovery IN text_01\n",
      "se agregó:  1.0 en:  0 36\n",
      "discovery NOT IN text_02\n",
      "se agregó:  0.0 en:  1 36\n",
      "discovery NOT IN text_03\n",
      "se agregó:  0.0 en:  2 36\n",
      "criticism IN text_01\n",
      "se agregó:  1.0 en:  0 37\n",
      "criticism NOT IN text_02\n",
      "se agregó:  0.0 en:  1 37\n",
      "criticism NOT IN text_03\n",
      "se agregó:  0.0 en:  2 37\n",
      "explanation IN text_01\n",
      "se agregó:  1.0 en:  0 38\n",
      "explanation NOT IN text_02\n",
      "se agregó:  0.0 en:  1 38\n",
      "explanation NOT IN text_03\n",
      "se agregó:  0.0 en:  2 38\n",
      "allocation IN text_01\n",
      "se agregó:  1.0 en:  0 39\n",
      "allocation NOT IN text_02\n",
      "se agregó:  0.0 en:  1 39\n",
      "allocation NOT IN text_03\n",
      "se agregó:  0.0 en:  2 39\n",
      "computational IN text_01\n",
      "se agregó:  1.0 en:  0 40\n",
      "computational NOT IN text_02\n",
      "se agregó:  0.0 en:  1 40\n",
      "computational NOT IN text_03\n",
      "se agregó:  0.0 en:  2 40\n",
      "resources IN text_01\n",
      "se agregó:  1.0 en:  0 41\n",
      "resources NOT IN text_02\n",
      "se agregó:  0.0 en:  1 41\n",
      "resources NOT IN text_03\n",
      "se agregó:  0.0 en:  2 41\n",
      "hyperparameter IN text_01\n",
      "se agregó:  1.0 en:  0 42\n",
      "hyperparameter NOT IN text_02\n",
      "se agregó:  0.0 en:  1 42\n",
      "hyperparameter NOT IN text_03\n",
      "se agregó:  0.0 en:  2 42\n",
      "optimization IN text_01\n",
      "se agregó:  1.0 en:  0 43\n",
      "optimization NOT IN text_02\n",
      "se agregó:  0.0 en:  1 43\n",
      "optimization NOT IN text_03\n",
      "se agregó:  0.0 en:  2 43\n",
      "inference IN text_01\n",
      "se agregó:  1.0 en:  0 44\n",
      "inference NOT IN text_02\n",
      "se agregó:  0.0 en:  1 44\n",
      "inference NOT IN text_03\n",
      "se agregó:  0.0 en:  2 44\n",
      "monitoring IN text_01\n",
      "se agregó:  1.0 en:  0 45\n",
      "monitoring NOT IN text_02\n",
      "se agregó:  0.0 en:  1 45\n",
      "monitoring NOT IN text_03\n",
      "se agregó:  0.0 en:  2 45\n",
      "anomaly IN text_01\n",
      "se agregó:  1.0 en:  0 46\n",
      "anomaly NOT IN text_02\n",
      "se agregó:  0.0 en:  1 46\n",
      "anomaly NOT IN text_03\n",
      "se agregó:  0.0 en:  2 46\n",
      "detection IN text_01\n",
      "se agregó:  1.0 en:  0 47\n",
      "detection NOT IN text_02\n",
      "se agregó:  0.0 en:  1 47\n",
      "detection NOT IN text_03\n",
      "se agregó:  0.0 en:  2 47\n",
      "huge IN text_01\n",
      "se agregó:  1.0 en:  0 48\n",
      "huge NOT IN text_02\n",
      "se agregó:  0.0 en:  1 48\n",
      "huge NOT IN text_03\n",
      "se agregó:  0.0 en:  2 48\n",
      "list IN text_01\n",
      "se agregó:  1.0 en:  0 49\n",
      "list NOT IN text_02\n",
      "se agregó:  0.0 en:  1 49\n",
      "list NOT IN text_03\n",
      "se agregó:  0.0 en:  2 49\n",
      "things IN text_01\n",
      "se agregó:  1.0 en:  0 50\n",
      "things NOT IN text_02\n",
      "se agregó:  0.0 en:  1 50\n",
      "things NOT IN text_03\n",
      "se agregó:  0.0 en:  2 50\n",
      "optimally IN text_01\n",
      "se agregó:  1.0 en:  0 51\n",
      "optimally NOT IN text_02\n",
      "se agregó:  0.0 en:  1 51\n",
      "optimally NOT IN text_03\n",
      "se agregó:  0.0 en:  2 51\n",
      "like IN text_01\n",
      "se agregó:  1.0 en:  0 52\n",
      "like NOT IN text_02\n",
      "se agregó:  0.0 en:  1 52\n",
      "like NOT IN text_03\n",
      "se agregó:  0.0 en:  2 52\n",
      "caveat NOT IN text_01\n",
      "se agregó:  0.0 en:  0 53\n",
      "caveat IN text_02\n",
      "se agregó:  1.0 en:  1 53\n",
      "caveat NOT IN text_03\n",
      "se agregó:  0.0 en:  2 53\n",
      "course NOT IN text_01\n",
      "se agregó:  0.0 en:  0 54\n",
      "course IN text_02\n",
      "se agregó:  1.0 en:  1 54\n",
      "course NOT IN text_03\n",
      "se agregó:  0.0 en:  2 54\n",
      "full NOT IN text_01\n",
      "se agregó:  0.0 en:  0 55\n",
      "full IN text_02\n",
      "se agregó:  1.0 en:  1 55\n",
      "full NOT IN text_03\n",
      "se agregó:  0.0 en:  2 55\n",
      "automation NOT IN text_01\n",
      "se agregó:  0.0 en:  0 56\n",
      "automation IN text_02\n",
      "se agregó:  1.0 en:  1 56\n",
      "automation IN text_03\n",
      "se agregó:  1.0 en:  2 56\n",
      "motivate NOT IN text_01\n",
      "se agregó:  0.0 en:  0 57\n",
      "motivate IN text_02\n",
      "se agregó:  1.0 en:  1 57\n",
      "motivate NOT IN text_03\n",
      "se agregó:  0.0 en:  2 57\n",
      "scientific NOT IN text_01\n",
      "se agregó:  0.0 en:  0 58\n",
      "scientific IN text_02\n",
      "se agregó:  1.0 en:  1 58\n",
      "scientific NOT IN text_03\n",
      "se agregó:  0.0 en:  2 58\n",
      "research NOT IN text_01\n",
      "se agregó:  0.0 en:  0 59\n",
      "research IN text_02\n",
      "se agregó:  1.0 en:  1 59\n",
      "research NOT IN text_03\n",
      "se agregó:  0.0 en:  2 59\n",
      "provide NOT IN text_01\n",
      "se agregó:  0.0 en:  0 60\n",
      "provide IN text_02\n",
      "se agregó:  1.0 en:  1 60\n",
      "provide NOT IN text_03\n",
      "se agregó:  0.0 en:  2 60\n",
      "long NOT IN text_01\n",
      "se agregó:  0.0 en:  0 61\n",
      "long IN text_02\n",
      "se agregó:  1.0 en:  1 61\n",
      "long NOT IN text_03\n",
      "se agregó:  0.0 en:  2 61\n",
      "term NOT IN text_01\n",
      "se agregó:  0.0 en:  0 62\n",
      "term IN text_02\n",
      "se agregó:  1.0 en:  1 62\n",
      "term NOT IN text_03\n",
      "se agregó:  0.0 en:  2 62\n",
      "engineering NOT IN text_01\n",
      "se agregó:  0.0 en:  0 63\n",
      "engineering IN text_02\n",
      "se agregó:  1.0 en:  1 63\n",
      "engineering NOT IN text_03\n",
      "se agregó:  0.0 en:  2 63\n",
      "goal NOT IN text_01\n",
      "se agregó:  0.0 en:  0 64\n",
      "goal IN text_02\n",
      "se agregó:  1.0 en:  1 64\n",
      "goal IN text_03\n",
      "se agregó:  1.0 en:  2 64\n",
      "practice NOT IN text_01\n",
      "se agregó:  0.0 en:  0 65\n",
      "practice IN text_02\n",
      "se agregó:  1.0 en:  1 65\n",
      "practice NOT IN text_03\n",
      "se agregó:  0.0 en:  2 65\n",
      "probably NOT IN text_01\n",
      "se agregó:  0.0 en:  0 66\n",
      "probably IN text_02\n",
      "se agregó:  1.0 en:  1 66\n",
      "probably NOT IN text_03\n",
      "se agregó:  0.0 en:  2 66\n",
      "semiautomate NOT IN text_01\n",
      "se agregó:  0.0 en:  0 67\n",
      "semiautomate IN text_02\n",
      "se agregó:  1.0 en:  1 67\n",
      "semiautomate NOT IN text_03\n",
      "se agregó:  0.0 en:  2 67\n",
      "gradually NOT IN text_01\n",
      "se agregó:  0.0 en:  0 68\n",
      "gradually IN text_02\n",
      "se agregó:  1.0 en:  1 68\n",
      "gradually NOT IN text_03\n",
      "se agregó:  0.0 en:  2 68\n",
      "remove NOT IN text_01\n",
      "se agregó:  0.0 en:  0 69\n",
      "remove IN text_02\n",
      "se agregó:  1.0 en:  1 69\n",
      "remove NOT IN text_03\n",
      "se agregó:  0.0 en:  2 69\n",
      "human NOT IN text_01\n",
      "se agregó:  0.0 en:  0 70\n",
      "human IN text_02\n",
      "se agregó:  1.0 en:  1 70\n",
      "human IN text_03\n",
      "se agregó:  1.0 en:  2 70\n",
      "loop NOT IN text_01\n",
      "se agregó:  0.0 en:  0 71\n",
      "loop IN text_02\n",
      "se agregó:  1.0 en:  1 71\n",
      "loop NOT IN text_03\n",
      "se agregó:  0.0 en:  2 71\n",
      "needed NOT IN text_01\n",
      "se agregó:  0.0 en:  0 72\n",
      "needed IN text_02\n",
      "se agregó:  1.0 en:  1 72\n",
      "needed NOT IN text_03\n",
      "se agregó:  0.0 en:  2 72\n",
      "along NOT IN text_01\n",
      "se agregó:  0.0 en:  0 73\n",
      "along IN text_02\n",
      "se agregó:  1.0 en:  1 73\n",
      "along NOT IN text_03\n",
      "se agregó:  0.0 en:  2 73\n",
      "way NOT IN text_01\n",
      "se agregó:  0.0 en:  0 74\n",
      "way IN text_02\n",
      "se agregó:  1.0 en:  1 74\n",
      "way NOT IN text_03\n",
      "se agregó:  0.0 en:  2 74\n",
      "going NOT IN text_01\n",
      "se agregó:  0.0 en:  0 75\n",
      "going IN text_02\n",
      "se agregó:  1.0 en:  1 75\n",
      "going IN text_03\n",
      "se agregó:  1.0 en:  2 75\n",
      "happen NOT IN text_01\n",
      "se agregó:  0.0 en:  0 76\n",
      "happen IN text_02\n",
      "se agregó:  1.0 en:  1 76\n",
      "happen NOT IN text_03\n",
      "se agregó:  0.0 en:  2 76\n",
      "likely NOT IN text_01\n",
      "se agregó:  0.0 en:  0 77\n",
      "likely IN text_02\n",
      "se agregó:  1.0 en:  1 77\n",
      "likely IN text_03\n",
      "se agregó:  1.0 en:  2 77\n",
      "develop NOT IN text_01\n",
      "se agregó:  0.0 en:  0 78\n",
      "develop IN text_02\n",
      "se agregó:  1.0 en:  1 78\n",
      "develop NOT IN text_03\n",
      "se agregó:  0.0 en:  2 78\n",
      "powerful NOT IN text_01\n",
      "se agregó:  0.0 en:  0 79\n",
      "powerful IN text_02\n",
      "se agregó:  1.0 en:  1 79\n",
      "powerful NOT IN text_03\n",
      "se agregó:  0.0 en:  2 79\n",
      "tools NOT IN text_01\n",
      "se agregó:  0.0 en:  0 80\n",
      "tools IN text_02\n",
      "se agregó:  1.0 en:  1 80\n",
      "tools NOT IN text_03\n",
      "se agregó:  0.0 en:  2 80\n",
      "help NOT IN text_01\n",
      "se agregó:  0.0 en:  0 81\n",
      "help IN text_02\n",
      "se agregó:  1.0 en:  1 81\n",
      "help NOT IN text_03\n",
      "se agregó:  0.0 en:  2 81\n",
      "make NOT IN text_01\n",
      "se agregó:  0.0 en:  0 82\n",
      "make IN text_02\n",
      "se agregó:  1.0 en:  1 82\n",
      "make NOT IN text_03\n",
      "se agregó:  0.0 en:  2 82\n",
      "first NOT IN text_01\n",
      "se agregó:  0.0 en:  0 83\n",
      "first IN text_02\n",
      "se agregó:  1.0 en:  1 83\n",
      "first NOT IN text_03\n",
      "se agregó:  0.0 en:  2 83\n",
      "systematic NOT IN text_01\n",
      "se agregó:  0.0 en:  0 84\n",
      "systematic IN text_02\n",
      "se agregó:  1.0 en:  1 84\n",
      "systematic NOT IN text_03\n",
      "se agregó:  0.0 en:  2 84\n",
      "ad NOT IN text_01\n",
      "se agregó:  0.0 en:  0 85\n",
      "ad IN text_02\n",
      "se agregó:  1.0 en:  1 85\n",
      "ad NOT IN text_03\n",
      "se agregó:  0.0 en:  2 85\n",
      "hoc NOT IN text_01\n",
      "se agregó:  0.0 en:  0 86\n",
      "hoc IN text_02\n",
      "se agregó:  1.0 en:  1 86\n",
      "hoc NOT IN text_03\n",
      "se agregó:  0.0 en:  2 86\n",
      "days NOT IN text_01\n",
      "se agregó:  0.0 en:  0 87\n",
      "days IN text_02\n",
      "se agregó:  1.0 en:  1 87\n",
      "days NOT IN text_03\n",
      "se agregó:  0.0 en:  2 87\n",
      "also NOT IN text_01\n",
      "se agregó:  0.0 en:  0 88\n",
      "also IN text_02\n",
      "se agregó:  1.0 en:  1 88\n",
      "also NOT IN text_03\n",
      "se agregó:  0.0 en:  2 88\n",
      "efficient NOT IN text_01\n",
      "se agregó:  0.0 en:  0 89\n",
      "efficient IN text_02\n",
      "se agregó:  1.0 en:  1 89\n",
      "efficient NOT IN text_03\n",
      "se agregó:  0.0 en:  2 89\n",
      "worthy NOT IN text_01\n",
      "se agregó:  0.0 en:  0 90\n",
      "worthy NOT IN text_02\n",
      "se agregó:  0.0 en:  1 90\n",
      "worthy IN text_03\n",
      "se agregó:  1.0 en:  2 90\n",
      "goals NOT IN text_01\n",
      "se agregó:  0.0 en:  0 91\n",
      "goals NOT IN text_02\n",
      "se agregó:  0.0 en:  1 91\n",
      "goals IN text_03\n",
      "se agregó:  1.0 en:  2 91\n",
      "even NOT IN text_01\n",
      "se agregó:  0.0 en:  0 92\n",
      "even NOT IN text_02\n",
      "se agregó:  0.0 en:  1 92\n",
      "even IN text_03\n",
      "se agregó:  1.0 en:  2 92\n",
      "succeed NOT IN text_01\n",
      "se agregó:  0.0 en:  0 93\n",
      "succeed NOT IN text_02\n",
      "se agregó:  0.0 en:  1 93\n",
      "succeed IN text_03\n",
      "se agregó:  1.0 en:  2 93\n",
      "final NOT IN text_01\n",
      "se agregó:  0.0 en:  0 94\n",
      "final NOT IN text_02\n",
      "se agregó:  0.0 en:  1 94\n",
      "final IN text_03\n",
      "se agregó:  1.0 en:  2 94\n",
      "book NOT IN text_01\n",
      "se agregó:  0.0 en:  0 95\n",
      "book NOT IN text_02\n",
      "se agregó:  0.0 en:  1 95\n",
      "book IN text_03\n",
      "se agregó:  1.0 en:  2 95\n",
      "demonstrates NOT IN text_01\n",
      "se agregó:  0.0 en:  0 96\n",
      "demonstrates NOT IN text_02\n",
      "se agregó:  0.0 en:  1 96\n",
      "demonstrates IN text_03\n",
      "se agregó:  1.0 en:  2 96\n",
      "current NOT IN text_01\n",
      "se agregó:  0.0 en:  0 97\n",
      "current NOT IN text_02\n",
      "se agregó:  0.0 en:  1 97\n",
      "current IN text_03\n",
      "se agregó:  1.0 en:  2 97\n",
      "automl NOT IN text_01\n",
      "se agregó:  0.0 en:  0 98\n",
      "automl NOT IN text_02\n",
      "se agregó:  0.0 en:  1 98\n",
      "automl IN text_03\n",
      "se agregó:  1.0 en:  2 98\n",
      "methods NOT IN text_01\n",
      "se agregó:  0.0 en:  0 99\n",
      "methods NOT IN text_02\n",
      "se agregó:  0.0 en:  1 99\n",
      "methods IN text_03\n",
      "se agregó:  1.0 en:  2 99\n",
      "already NOT IN text_01\n",
      "se agregó:  0.0 en:  0 100\n",
      "already NOT IN text_02\n",
      "se agregó:  0.0 en:  1 100\n",
      "already IN text_03\n",
      "se agregó:  1.0 en:  2 100\n",
      "surpass NOT IN text_01\n",
      "se agregó:  0.0 en:  0 101\n",
      "surpass NOT IN text_02\n",
      "se agregó:  0.0 en:  1 101\n",
      "surpass IN text_03\n",
      "se agregó:  1.0 en:  2 101\n",
      "experts NOT IN text_01\n",
      "se agregó:  0.0 en:  0 102\n",
      "experts NOT IN text_02\n",
      "se agregó:  0.0 en:  1 102\n",
      "experts IN text_03\n",
      "se agregó:  1.0 en:  2 102\n",
      "several NOT IN text_01\n",
      "se agregó:  0.0 en:  0 103\n",
      "several NOT IN text_02\n",
      "se agregó:  0.0 en:  1 103\n",
      "several IN text_03\n",
      "se agregó:  1.0 en:  2 103\n",
      "tasks NOT IN text_01\n",
      "se agregó:  0.0 en:  0 104\n",
      "tasks NOT IN text_02\n",
      "se agregó:  0.0 en:  1 104\n",
      "tasks IN text_03\n",
      "se agregó:  1.0 en:  2 104\n",
      "trend NOT IN text_01\n",
      "se agregó:  0.0 en:  0 105\n",
      "trend NOT IN text_02\n",
      "se agregó:  0.0 en:  1 105\n",
      "trend IN text_03\n",
      "se agregó:  1.0 en:  2 105\n",
      "intensify NOT IN text_01\n",
      "se agregó:  0.0 en:  0 106\n",
      "intensify NOT IN text_02\n",
      "se agregó:  0.0 en:  1 106\n",
      "intensify IN text_03\n",
      "se agregó:  1.0 en:  2 106\n",
      "making NOT IN text_01\n",
      "se agregó:  0.0 en:  0 107\n",
      "making NOT IN text_02\n",
      "se agregó:  0.0 en:  1 107\n",
      "making IN text_03\n",
      "se agregó:  1.0 en:  2 107\n",
      "progress NOT IN text_01\n",
      "se agregó:  0.0 en:  0 108\n",
      "progress NOT IN text_02\n",
      "se agregó:  0.0 en:  1 108\n",
      "progress IN text_03\n",
      "se agregó:  1.0 en:  2 108\n",
      "computation NOT IN text_01\n",
      "se agregó:  0.0 en:  0 109\n",
      "computation NOT IN text_02\n",
      "se agregó:  0.0 en:  1 109\n",
      "computation IN text_03\n",
      "se agregó:  1.0 en:  2 109\n",
      "becomes NOT IN text_01\n",
      "se agregó:  0.0 en:  0 110\n",
      "becomes NOT IN text_02\n",
      "se agregó:  0.0 en:  1 110\n",
      "becomes IN text_03\n",
      "se agregó:  1.0 en:  2 110\n",
      "cheaper NOT IN text_01\n",
      "se agregó:  0.0 en:  0 111\n",
      "cheaper NOT IN text_02\n",
      "se agregó:  0.0 en:  1 111\n",
      "cheaper IN text_03\n",
      "se agregó:  1.0 en:  2 111\n",
      "therefore NOT IN text_01\n",
      "se agregó:  0.0 en:  0 112\n",
      "therefore NOT IN text_02\n",
      "se agregó:  0.0 en:  1 112\n",
      "therefore IN text_03\n",
      "se agregó:  1.0 en:  2 112\n",
      "clearly NOT IN text_01\n",
      "se agregó:  0.0 en:  0 113\n",
      "clearly NOT IN text_02\n",
      "se agregó:  0.0 en:  1 113\n",
      "clearly IN text_03\n",
      "se agregó:  1.0 en:  2 113\n",
      "one NOT IN text_01\n",
      "se agregó:  0.0 en:  0 114\n",
      "one NOT IN text_02\n",
      "se agregó:  0.0 en:  1 114\n",
      "one IN text_03\n",
      "se agregó:  1.0 en:  2 114\n",
      "topics NOT IN text_01\n",
      "se agregó:  0.0 en:  0 115\n",
      "topics NOT IN text_02\n",
      "se agregó:  0.0 en:  1 115\n",
      "topics IN text_03\n",
      "se agregó:  1.0 en:  2 115\n",
      "stay NOT IN text_01\n",
      "se agregó:  0.0 en:  0 116\n",
      "stay NOT IN text_02\n",
      "se agregó:  0.0 en:  1 116\n",
      "stay IN text_03\n",
      "se agregó:  1.0 en:  2 116\n",
      "great NOT IN text_01\n",
      "se agregó:  0.0 en:  0 117\n",
      "great NOT IN text_02\n",
      "se agregó:  0.0 en:  1 117\n",
      "great IN text_03\n",
      "se agregó:  1.0 en:  2 117\n",
      "time NOT IN text_01\n",
      "se agregó:  0.0 en:  0 118\n",
      "time NOT IN text_02\n",
      "se agregó:  0.0 en:  1 118\n",
      "time IN text_03\n",
      "se agregó:  1.0 en:  2 118\n",
      "get NOT IN text_01\n",
      "se agregó:  0.0 en:  0 119\n",
      "get NOT IN text_02\n",
      "se agregó:  0.0 en:  1 119\n",
      "get IN text_03\n",
      "se agregó:  1.0 en:  2 119\n",
      "involved NOT IN text_01\n",
      "se agregó:  0.0 en:  0 120\n",
      "involved NOT IN text_02\n",
      "se agregó:  0.0 en:  1 120\n",
      "involved IN text_03\n",
      "se agregó:  1.0 en:  2 120\n",
      "excellent NOT IN text_01\n",
      "se agregó:  0.0 en:  0 121\n",
      "excellent NOT IN text_02\n",
      "se agregó:  0.0 en:  1 121\n",
      "excellent IN text_03\n",
      "se agregó:  1.0 en:  2 121\n",
      "starting NOT IN text_01\n",
      "se agregó:  0.0 en:  0 122\n",
      "starting NOT IN text_02\n",
      "se agregó:  0.0 en:  1 122\n",
      "starting IN text_03\n",
      "se agregó:  1.0 en:  2 122\n",
      "point NOT IN text_01\n",
      "se agregó:  0.0 en:  0 123\n",
      "point NOT IN text_02\n",
      "se agregó:  0.0 en:  1 123\n",
      "point IN text_03\n",
      "se agregó:  1.0 en:  2 123\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "\n",
    "for word_termns in dicc_termns: #dicc_termns todos los términos\n",
    "#    print()\n",
    "    for word_texts in dicc_texts: #dicc_texts todos los textos\n",
    "#        print(\"EVALUAR:\", word_termns, \"EN: \", word_texts)\n",
    "        if(word_termns in dicc_texts[word_texts]): #si está\n",
    "            print(word_termns, \"IN\", word_texts)\n",
    "            \n",
    "            matrix[j, i] = 1\n",
    "            \n",
    "        elif(word_termns not in dicc_texts[word_texts]): # si no está\n",
    "            print(word_termns, \"NOT IN\", word_texts)\n",
    "            \n",
    "            matrix[j, i] = 0\n",
    "            \n",
    "            \n",
    "        print(\"se agregó: \", matrix[j,i], \"en: \", j, i)\n",
    "            \n",
    "        j = j + 1\n",
    "        \n",
    "    j = 0\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 124)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10597597584982435"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_cos_t01_t02 = dot(matrix[0],matrix[1])/(norm(matrix[0])*norm(matrix[1]))\n",
    "bin_cos_t01_t02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_cos_t02_t03 = dot(matrix[1],matrix[2])/(norm(matrix[1])*norm(matrix[2]))\n",
    "bin_cos_t02_t03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06358558550989461"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_cos_t01_t03 = dot(matrix[0],matrix[2])/(norm(matrix[0])*norm(matrix[2]))\n",
    "bin_cos_t01_t03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matriz Término Documento (frecuencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.zeros((len(dicc_texts), len(dicc_termns))) # Pre-allocate matrix\n",
    "#matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passionate IN text_01\n",
      "se agregó:  1.0 en:  0 0\n",
      "passionate NOT IN text_02\n",
      "se agregó:  0.0 en:  1 0\n",
      "passionate NOT IN text_03\n",
      "se agregó:  0.0 en:  2 0\n",
      "automating IN text_01\n",
      "se agregó:  9.0 en:  0 1\n",
      "automating NOT IN text_02\n",
      "se agregó:  0.0 en:  1 1\n",
      "automating NOT IN text_03\n",
      "se agregó:  0.0 en:  2 1\n",
      "machine IN text_01\n",
      "se agregó:  4.0 en:  0 2\n",
      "machine IN text_02\n",
      "se agregó:  4.0 en:  1 2\n",
      "machine IN text_03\n",
      "se agregó:  4.0 en:  2 2\n",
      "learning IN text_01\n",
      "se agregó:  4.0 en:  0 3\n",
      "learning IN text_02\n",
      "se agregó:  4.0 en:  1 3\n",
      "learning IN text_03\n",
      "se agregó:  4.0 en:  2 3\n",
      "ever IN text_01\n",
      "se agregó:  2.0 en:  0 4\n",
      "ever NOT IN text_02\n",
      "se agregó:  0.0 en:  1 4\n",
      "ever IN text_03\n",
      "se agregó:  2.0 en:  2 4\n",
      "since IN text_01\n",
      "se agregó:  2.0 en:  0 5\n",
      "since IN text_02\n",
      "se agregó:  2.0 en:  1 5\n",
      "since NOT IN text_03\n",
      "se agregó:  0.0 en:  2 5\n",
      "automatic IN text_01\n",
      "se agregó:  1.0 en:  0 6\n",
      "automatic NOT IN text_02\n",
      "se agregó:  0.0 en:  1 6\n",
      "automatic NOT IN text_03\n",
      "se agregó:  0.0 en:  2 6\n",
      "statistician IN text_01\n",
      "se agregó:  1.0 en:  0 7\n",
      "statistician NOT IN text_02\n",
      "se agregó:  0.0 en:  1 7\n",
      "statistician NOT IN text_03\n",
      "se agregó:  0.0 en:  2 7\n",
      "project IN text_01\n",
      "se agregó:  1.0 en:  0 8\n",
      "project NOT IN text_02\n",
      "se agregó:  0.0 en:  1 8\n",
      "project NOT IN text_03\n",
      "se agregó:  0.0 en:  2 8\n",
      "started IN text_01\n",
      "se agregó:  1.0 en:  0 9\n",
      "started NOT IN text_02\n",
      "se agregó:  0.0 en:  1 9\n",
      "started NOT IN text_03\n",
      "se agregó:  0.0 en:  2 9\n",
      "back IN text_01\n",
      "se agregó:  1.0 en:  0 10\n",
      "back NOT IN text_02\n",
      "se agregó:  0.0 en:  1 10\n",
      "back NOT IN text_03\n",
      "se agregó:  0.0 en:  2 10\n",
      "2014 IN text_01\n",
      "se agregó:  1.0 en:  0 11\n",
      "2014 NOT IN text_02\n",
      "se agregó:  0.0 en:  1 11\n",
      "2014 NOT IN text_03\n",
      "se agregó:  0.0 en:  2 11\n",
      "want IN text_01\n",
      "se agregó:  2.0 en:  0 12\n",
      "want IN text_02\n",
      "se agregó:  2.0 en:  1 12\n",
      "want NOT IN text_03\n",
      "se agregó:  0.0 en:  2 12\n",
      "us IN text_01\n",
      "se agregó:  1.0 en:  0 13\n",
      "us NOT IN text_02\n",
      "se agregó:  0.0 en:  1 13\n",
      "us NOT IN text_03\n",
      "se agregó:  0.0 en:  2 13\n",
      "really IN text_01\n",
      "se agregó:  1.0 en:  0 14\n",
      "really NOT IN text_02\n",
      "se agregó:  0.0 en:  1 14\n",
      "really NOT IN text_03\n",
      "se agregó:  0.0 en:  2 14\n",
      "ambitious IN text_01\n",
      "se agregó:  1.0 en:  0 15\n",
      "ambitious NOT IN text_02\n",
      "se agregó:  0.0 en:  1 15\n",
      "ambitious NOT IN text_03\n",
      "se agregó:  0.0 en:  2 15\n",
      "endeavor IN text_01\n",
      "se agregó:  1.0 en:  0 16\n",
      "endeavor NOT IN text_02\n",
      "se agregó:  0.0 en:  1 16\n",
      "endeavor NOT IN text_03\n",
      "se agregó:  0.0 en:  2 16\n",
      "try IN text_01\n",
      "se agregó:  2.0 en:  0 17\n",
      "try IN text_02\n",
      "se agregó:  2.0 en:  1 17\n",
      "try NOT IN text_03\n",
      "se agregó:  0.0 en:  2 17\n",
      "automate IN text_01\n",
      "se agregó:  2.0 en:  0 18\n",
      "automate NOT IN text_02\n",
      "se agregó:  0.0 en:  1 18\n",
      "automate NOT IN text_03\n",
      "se agregó:  0.0 en:  2 18\n",
      "aspects IN text_01\n",
      "se agregó:  1.0 en:  0 19\n",
      "aspects NOT IN text_02\n",
      "se agregó:  0.0 en:  1 19\n",
      "aspects NOT IN text_03\n",
      "se agregó:  0.0 en:  2 19\n",
      "entire IN text_01\n",
      "se agregó:  1.0 en:  0 20\n",
      "entire NOT IN text_02\n",
      "se agregó:  0.0 en:  1 20\n",
      "entire NOT IN text_03\n",
      "se agregó:  0.0 en:  2 20\n",
      "data IN text_01\n",
      "se agregó:  4.0 en:  0 21\n",
      "data NOT IN text_02\n",
      "se agregó:  0.0 en:  1 21\n",
      "data NOT IN text_03\n",
      "se agregó:  0.0 en:  2 21\n",
      "analysis IN text_01\n",
      "se agregó:  1.0 en:  0 22\n",
      "analysis NOT IN text_02\n",
      "se agregó:  0.0 en:  1 22\n",
      "analysis NOT IN text_03\n",
      "se agregó:  0.0 en:  2 22\n",
      "pipeline IN text_01\n",
      "se agregó:  1.0 en:  0 23\n",
      "pipeline NOT IN text_02\n",
      "se agregó:  0.0 en:  1 23\n",
      "pipeline NOT IN text_03\n",
      "se agregó:  0.0 en:  2 23\n",
      "includes IN text_01\n",
      "se agregó:  1.0 en:  0 24\n",
      "includes NOT IN text_02\n",
      "se agregó:  0.0 en:  1 24\n",
      "includes NOT IN text_03\n",
      "se agregó:  0.0 en:  2 24\n",
      "collection IN text_01\n",
      "se agregó:  1.0 en:  0 25\n",
      "collection NOT IN text_02\n",
      "se agregó:  0.0 en:  1 25\n",
      "collection NOT IN text_03\n",
      "se agregó:  0.0 en:  2 25\n",
      "experiment IN text_01\n",
      "se agregó:  1.0 en:  0 26\n",
      "experiment NOT IN text_02\n",
      "se agregó:  0.0 en:  1 26\n",
      "experiment NOT IN text_03\n",
      "se agregó:  0.0 en:  2 26\n",
      "design IN text_01\n",
      "se agregó:  1.0 en:  0 27\n",
      "design NOT IN text_02\n",
      "se agregó:  0.0 en:  1 27\n",
      "design NOT IN text_03\n",
      "se agregó:  0.0 en:  2 27\n",
      "cleanup IN text_01\n",
      "se agregó:  1.0 en:  0 28\n",
      "cleanup NOT IN text_02\n",
      "se agregó:  0.0 en:  1 28\n",
      "cleanup NOT IN text_03\n",
      "se agregó:  0.0 en:  2 28\n",
      "missing IN text_01\n",
      "se agregó:  1.0 en:  0 29\n",
      "missing NOT IN text_02\n",
      "se agregó:  0.0 en:  1 29\n",
      "missing NOT IN text_03\n",
      "se agregó:  0.0 en:  2 29\n",
      "imputa IN text_01\n",
      "se agregó:  1.0 en:  0 30\n",
      "imputa NOT IN text_02\n",
      "se agregó:  0.0 en:  1 30\n",
      "imputa NOT IN text_03\n",
      "se agregó:  0.0 en:  2 30\n",
      "tion IN text_01\n",
      "se agregó:  1.0 en:  0 31\n",
      "tion NOT IN text_02\n",
      "se agregó:  0.0 en:  1 31\n",
      "tion NOT IN text_03\n",
      "se agregó:  0.0 en:  2 31\n",
      "feature IN text_01\n",
      "se agregó:  1.0 en:  0 32\n",
      "feature NOT IN text_02\n",
      "se agregó:  0.0 en:  1 32\n",
      "feature NOT IN text_03\n",
      "se agregó:  0.0 en:  2 32\n",
      "selection IN text_01\n",
      "se agregó:  1.0 en:  0 33\n",
      "selection NOT IN text_02\n",
      "se agregó:  0.0 en:  1 33\n",
      "selection NOT IN text_03\n",
      "se agregó:  0.0 en:  2 33\n",
      "transformation IN text_01\n",
      "se agregó:  1.0 en:  0 34\n",
      "transformation NOT IN text_02\n",
      "se agregó:  0.0 en:  1 34\n",
      "transformation NOT IN text_03\n",
      "se agregó:  0.0 en:  2 34\n",
      "model IN text_01\n",
      "se agregó:  2.0 en:  0 35\n",
      "model NOT IN text_02\n",
      "se agregó:  0.0 en:  1 35\n",
      "model NOT IN text_03\n",
      "se agregó:  0.0 en:  2 35\n",
      "discovery IN text_01\n",
      "se agregó:  1.0 en:  0 36\n",
      "discovery NOT IN text_02\n",
      "se agregó:  0.0 en:  1 36\n",
      "discovery NOT IN text_03\n",
      "se agregó:  0.0 en:  2 36\n",
      "criticism IN text_01\n",
      "se agregó:  1.0 en:  0 37\n",
      "criticism NOT IN text_02\n",
      "se agregó:  0.0 en:  1 37\n",
      "criticism NOT IN text_03\n",
      "se agregó:  0.0 en:  2 37\n",
      "explanation IN text_01\n",
      "se agregó:  1.0 en:  0 38\n",
      "explanation NOT IN text_02\n",
      "se agregó:  0.0 en:  1 38\n",
      "explanation NOT IN text_03\n",
      "se agregó:  0.0 en:  2 38\n",
      "allocation IN text_01\n",
      "se agregó:  1.0 en:  0 39\n",
      "allocation NOT IN text_02\n",
      "se agregó:  0.0 en:  1 39\n",
      "allocation NOT IN text_03\n",
      "se agregó:  0.0 en:  2 39\n",
      "computational IN text_01\n",
      "se agregó:  1.0 en:  0 40\n",
      "computational NOT IN text_02\n",
      "se agregó:  0.0 en:  1 40\n",
      "computational NOT IN text_03\n",
      "se agregó:  0.0 en:  2 40\n",
      "resources IN text_01\n",
      "se agregó:  1.0 en:  0 41\n",
      "resources NOT IN text_02\n",
      "se agregó:  0.0 en:  1 41\n",
      "resources NOT IN text_03\n",
      "se agregó:  0.0 en:  2 41\n",
      "hyperparameter IN text_01\n",
      "se agregó:  1.0 en:  0 42\n",
      "hyperparameter NOT IN text_02\n",
      "se agregó:  0.0 en:  1 42\n",
      "hyperparameter NOT IN text_03\n",
      "se agregó:  0.0 en:  2 42\n",
      "optimization IN text_01\n",
      "se agregó:  1.0 en:  0 43\n",
      "optimization NOT IN text_02\n",
      "se agregó:  0.0 en:  1 43\n",
      "optimization NOT IN text_03\n",
      "se agregó:  0.0 en:  2 43\n",
      "inference IN text_01\n",
      "se agregó:  1.0 en:  0 44\n",
      "inference NOT IN text_02\n",
      "se agregó:  0.0 en:  1 44\n",
      "inference NOT IN text_03\n",
      "se agregó:  0.0 en:  2 44\n",
      "monitoring IN text_01\n",
      "se agregó:  1.0 en:  0 45\n",
      "monitoring NOT IN text_02\n",
      "se agregó:  0.0 en:  1 45\n",
      "monitoring NOT IN text_03\n",
      "se agregó:  0.0 en:  2 45\n",
      "anomaly IN text_01\n",
      "se agregó:  1.0 en:  0 46\n",
      "anomaly NOT IN text_02\n",
      "se agregó:  0.0 en:  1 46\n",
      "anomaly NOT IN text_03\n",
      "se agregó:  0.0 en:  2 46\n",
      "detection IN text_01\n",
      "se agregó:  1.0 en:  0 47\n",
      "detection NOT IN text_02\n",
      "se agregó:  0.0 en:  1 47\n",
      "detection NOT IN text_03\n",
      "se agregó:  0.0 en:  2 47\n",
      "huge IN text_01\n",
      "se agregó:  1.0 en:  0 48\n",
      "huge NOT IN text_02\n",
      "se agregó:  0.0 en:  1 48\n",
      "huge NOT IN text_03\n",
      "se agregó:  0.0 en:  2 48\n",
      "list IN text_01\n",
      "se agregó:  1.0 en:  0 49\n",
      "list NOT IN text_02\n",
      "se agregó:  0.0 en:  1 49\n",
      "list NOT IN text_03\n",
      "se agregó:  0.0 en:  2 49\n",
      "things IN text_01\n",
      "se agregó:  1.0 en:  0 50\n",
      "things NOT IN text_02\n",
      "se agregó:  0.0 en:  1 50\n",
      "things NOT IN text_03\n",
      "se agregó:  0.0 en:  2 50\n",
      "optimally IN text_01\n",
      "se agregó:  1.0 en:  0 51\n",
      "optimally NOT IN text_02\n",
      "se agregó:  0.0 en:  1 51\n",
      "optimally NOT IN text_03\n",
      "se agregó:  0.0 en:  2 51\n",
      "like IN text_01\n",
      "se agregó:  1.0 en:  0 52\n",
      "like NOT IN text_02\n",
      "se agregó:  0.0 en:  1 52\n",
      "like NOT IN text_03\n",
      "se agregó:  0.0 en:  2 52\n",
      "caveat NOT IN text_01\n",
      "se agregó:  0.0 en:  0 53\n",
      "caveat IN text_02\n",
      "se agregó:  1.0 en:  1 53\n",
      "caveat NOT IN text_03\n",
      "se agregó:  0.0 en:  2 53\n",
      "course NOT IN text_01\n",
      "se agregó:  0.0 en:  0 54\n",
      "course IN text_02\n",
      "se agregó:  1.0 en:  1 54\n",
      "course NOT IN text_03\n",
      "se agregó:  0.0 en:  2 54\n",
      "full NOT IN text_01\n",
      "se agregó:  0.0 en:  0 55\n",
      "full IN text_02\n",
      "se agregó:  1.0 en:  1 55\n",
      "full NOT IN text_03\n",
      "se agregó:  0.0 en:  2 55\n",
      "automation NOT IN text_01\n",
      "se agregó:  0.0 en:  0 56\n",
      "automation IN text_02\n",
      "se agregó:  3.0 en:  1 56\n",
      "automation IN text_03\n",
      "se agregó:  3.0 en:  2 56\n",
      "motivate NOT IN text_01\n",
      "se agregó:  0.0 en:  0 57\n",
      "motivate IN text_02\n",
      "se agregó:  1.0 en:  1 57\n",
      "motivate NOT IN text_03\n",
      "se agregó:  0.0 en:  2 57\n",
      "scientific NOT IN text_01\n",
      "se agregó:  0.0 en:  0 58\n",
      "scientific IN text_02\n",
      "se agregó:  1.0 en:  1 58\n",
      "scientific NOT IN text_03\n",
      "se agregó:  0.0 en:  2 58\n",
      "research NOT IN text_01\n",
      "se agregó:  0.0 en:  0 59\n",
      "research IN text_02\n",
      "se agregó:  1.0 en:  1 59\n",
      "research NOT IN text_03\n",
      "se agregó:  0.0 en:  2 59\n",
      "provide NOT IN text_01\n",
      "se agregó:  0.0 en:  0 60\n",
      "provide IN text_02\n",
      "se agregó:  1.0 en:  1 60\n",
      "provide NOT IN text_03\n",
      "se agregó:  0.0 en:  2 60\n",
      "long NOT IN text_01\n",
      "se agregó:  0.0 en:  0 61\n",
      "long IN text_02\n",
      "se agregó:  1.0 en:  1 61\n",
      "long NOT IN text_03\n",
      "se agregó:  0.0 en:  2 61\n",
      "term NOT IN text_01\n",
      "se agregó:  0.0 en:  0 62\n",
      "term IN text_02\n",
      "se agregó:  1.0 en:  1 62\n",
      "term NOT IN text_03\n",
      "se agregó:  0.0 en:  2 62\n",
      "engineering NOT IN text_01\n",
      "se agregó:  0.0 en:  0 63\n",
      "engineering IN text_02\n",
      "se agregó:  1.0 en:  1 63\n",
      "engineering NOT IN text_03\n",
      "se agregó:  0.0 en:  2 63\n",
      "goal NOT IN text_01\n",
      "se agregó:  0.0 en:  0 64\n",
      "goal IN text_02\n",
      "se agregó:  2.0 en:  1 64\n",
      "goal IN text_03\n",
      "se agregó:  2.0 en:  2 64\n",
      "practice NOT IN text_01\n",
      "se agregó:  0.0 en:  0 65\n",
      "practice IN text_02\n",
      "se agregó:  2.0 en:  1 65\n",
      "practice NOT IN text_03\n",
      "se agregó:  0.0 en:  2 65\n",
      "probably NOT IN text_01\n",
      "se agregó:  0.0 en:  0 66\n",
      "probably IN text_02\n",
      "se agregó:  1.0 en:  1 66\n",
      "probably NOT IN text_03\n",
      "se agregó:  0.0 en:  2 66\n",
      "semiautomate NOT IN text_01\n",
      "se agregó:  0.0 en:  0 67\n",
      "semiautomate IN text_02\n",
      "se agregó:  1.0 en:  1 67\n",
      "semiautomate NOT IN text_03\n",
      "se agregó:  0.0 en:  2 67\n",
      "gradually NOT IN text_01\n",
      "se agregó:  0.0 en:  0 68\n",
      "gradually IN text_02\n",
      "se agregó:  1.0 en:  1 68\n",
      "gradually NOT IN text_03\n",
      "se agregó:  0.0 en:  2 68\n",
      "remove NOT IN text_01\n",
      "se agregó:  0.0 en:  0 69\n",
      "remove IN text_02\n",
      "se agregó:  1.0 en:  1 69\n",
      "remove NOT IN text_03\n",
      "se agregó:  0.0 en:  2 69\n",
      "human NOT IN text_01\n",
      "se agregó:  0.0 en:  0 70\n",
      "human IN text_02\n",
      "se agregó:  2.0 en:  1 70\n",
      "human IN text_03\n",
      "se agregó:  2.0 en:  2 70\n",
      "loop NOT IN text_01\n",
      "se agregó:  0.0 en:  0 71\n",
      "loop IN text_02\n",
      "se agregó:  1.0 en:  1 71\n",
      "loop NOT IN text_03\n",
      "se agregó:  0.0 en:  2 71\n",
      "needed NOT IN text_01\n",
      "se agregó:  0.0 en:  0 72\n",
      "needed IN text_02\n",
      "se agregó:  1.0 en:  1 72\n",
      "needed NOT IN text_03\n",
      "se agregó:  0.0 en:  2 72\n",
      "along NOT IN text_01\n",
      "se agregó:  0.0 en:  0 73\n",
      "along IN text_02\n",
      "se agregó:  1.0 en:  1 73\n",
      "along NOT IN text_03\n",
      "se agregó:  0.0 en:  2 73\n",
      "way NOT IN text_01\n",
      "se agregó:  0.0 en:  0 74\n",
      "way IN text_02\n",
      "se agregó:  1.0 en:  1 74\n",
      "way NOT IN text_03\n",
      "se agregó:  0.0 en:  2 74\n",
      "going NOT IN text_01\n",
      "se agregó:  0.0 en:  0 75\n",
      "going IN text_02\n",
      "se agregó:  2.0 en:  1 75\n",
      "going IN text_03\n",
      "se agregó:  2.0 en:  2 75\n",
      "happen NOT IN text_01\n",
      "se agregó:  0.0 en:  0 76\n",
      "happen IN text_02\n",
      "se agregó:  1.0 en:  1 76\n",
      "happen NOT IN text_03\n",
      "se agregó:  0.0 en:  2 76\n",
      "likely NOT IN text_01\n",
      "se agregó:  0.0 en:  0 77\n",
      "likely IN text_02\n",
      "se agregó:  2.0 en:  1 77\n",
      "likely IN text_03\n",
      "se agregó:  2.0 en:  2 77\n",
      "develop NOT IN text_01\n",
      "se agregó:  0.0 en:  0 78\n",
      "develop IN text_02\n",
      "se agregó:  1.0 en:  1 78\n",
      "develop NOT IN text_03\n",
      "se agregó:  0.0 en:  2 78\n",
      "powerful NOT IN text_01\n",
      "se agregó:  0.0 en:  0 79\n",
      "powerful IN text_02\n",
      "se agregó:  1.0 en:  1 79\n",
      "powerful NOT IN text_03\n",
      "se agregó:  0.0 en:  2 79\n",
      "tools NOT IN text_01\n",
      "se agregó:  0.0 en:  0 80\n",
      "tools IN text_02\n",
      "se agregó:  1.0 en:  1 80\n",
      "tools NOT IN text_03\n",
      "se agregó:  0.0 en:  2 80\n",
      "help NOT IN text_01\n",
      "se agregó:  0.0 en:  0 81\n",
      "help IN text_02\n",
      "se agregó:  1.0 en:  1 81\n",
      "help NOT IN text_03\n",
      "se agregó:  0.0 en:  2 81\n",
      "make NOT IN text_01\n",
      "se agregó:  0.0 en:  0 82\n",
      "make IN text_02\n",
      "se agregó:  1.0 en:  1 82\n",
      "make NOT IN text_03\n",
      "se agregó:  0.0 en:  2 82\n",
      "first NOT IN text_01\n",
      "se agregó:  0.0 en:  0 83\n",
      "first IN text_02\n",
      "se agregó:  1.0 en:  1 83\n",
      "first NOT IN text_03\n",
      "se agregó:  0.0 en:  2 83\n",
      "systematic NOT IN text_01\n",
      "se agregó:  0.0 en:  0 84\n",
      "systematic IN text_02\n",
      "se agregó:  1.0 en:  1 84\n",
      "systematic NOT IN text_03\n",
      "se agregó:  0.0 en:  2 84\n",
      "ad NOT IN text_01\n",
      "se agregó:  0.0 en:  0 85\n",
      "ad IN text_02\n",
      "se agregó:  1.0 en:  1 85\n",
      "ad NOT IN text_03\n",
      "se agregó:  0.0 en:  2 85\n",
      "hoc NOT IN text_01\n",
      "se agregó:  0.0 en:  0 86\n",
      "hoc IN text_02\n",
      "se agregó:  1.0 en:  1 86\n",
      "hoc NOT IN text_03\n",
      "se agregó:  0.0 en:  2 86\n",
      "days NOT IN text_01\n",
      "se agregó:  0.0 en:  0 87\n",
      "days IN text_02\n",
      "se agregó:  1.0 en:  1 87\n",
      "days NOT IN text_03\n",
      "se agregó:  0.0 en:  2 87\n",
      "also NOT IN text_01\n",
      "se agregó:  0.0 en:  0 88\n",
      "also IN text_02\n",
      "se agregó:  1.0 en:  1 88\n",
      "also NOT IN text_03\n",
      "se agregó:  0.0 en:  2 88\n",
      "efficient NOT IN text_01\n",
      "se agregó:  0.0 en:  0 89\n",
      "efficient IN text_02\n",
      "se agregó:  1.0 en:  1 89\n",
      "efficient NOT IN text_03\n",
      "se agregó:  0.0 en:  2 89\n",
      "worthy NOT IN text_01\n",
      "se agregó:  0.0 en:  0 90\n",
      "worthy NOT IN text_02\n",
      "se agregó:  0.0 en:  1 90\n",
      "worthy IN text_03\n",
      "se agregó:  1.0 en:  2 90\n",
      "goals NOT IN text_01\n",
      "se agregó:  0.0 en:  0 91\n",
      "goals NOT IN text_02\n",
      "se agregó:  0.0 en:  1 91\n",
      "goals IN text_03\n",
      "se agregó:  1.0 en:  2 91\n",
      "even NOT IN text_01\n",
      "se agregó:  0.0 en:  0 92\n",
      "even NOT IN text_02\n",
      "se agregó:  0.0 en:  1 92\n",
      "even IN text_03\n",
      "se agregó:  1.0 en:  2 92\n",
      "succeed NOT IN text_01\n",
      "se agregó:  0.0 en:  0 93\n",
      "succeed NOT IN text_02\n",
      "se agregó:  0.0 en:  1 93\n",
      "succeed IN text_03\n",
      "se agregó:  1.0 en:  2 93\n",
      "final NOT IN text_01\n",
      "se agregó:  0.0 en:  0 94\n",
      "final NOT IN text_02\n",
      "se agregó:  0.0 en:  1 94\n",
      "final IN text_03\n",
      "se agregó:  1.0 en:  2 94\n",
      "book NOT IN text_01\n",
      "se agregó:  0.0 en:  0 95\n",
      "book NOT IN text_02\n",
      "se agregó:  0.0 en:  1 95\n",
      "book IN text_03\n",
      "se agregó:  2.0 en:  2 95\n",
      "demonstrates NOT IN text_01\n",
      "se agregó:  0.0 en:  0 96\n",
      "demonstrates NOT IN text_02\n",
      "se agregó:  0.0 en:  1 96\n",
      "demonstrates IN text_03\n",
      "se agregó:  1.0 en:  2 96\n",
      "current NOT IN text_01\n",
      "se agregó:  0.0 en:  0 97\n",
      "current NOT IN text_02\n",
      "se agregó:  0.0 en:  1 97\n",
      "current IN text_03\n",
      "se agregó:  1.0 en:  2 97\n",
      "automl NOT IN text_01\n",
      "se agregó:  0.0 en:  0 98\n",
      "automl NOT IN text_02\n",
      "se agregó:  0.0 en:  1 98\n",
      "automl IN text_03\n",
      "se agregó:  3.0 en:  2 98\n",
      "methods NOT IN text_01\n",
      "se agregó:  0.0 en:  0 99\n",
      "methods NOT IN text_02\n",
      "se agregó:  0.0 en:  1 99\n",
      "methods IN text_03\n",
      "se agregó:  1.0 en:  2 99\n",
      "already NOT IN text_01\n",
      "se agregó:  0.0 en:  0 100\n",
      "already NOT IN text_02\n",
      "se agregó:  0.0 en:  1 100\n",
      "already IN text_03\n",
      "se agregó:  1.0 en:  2 100\n",
      "surpass NOT IN text_01\n",
      "se agregó:  0.0 en:  0 101\n",
      "surpass NOT IN text_02\n",
      "se agregó:  0.0 en:  1 101\n",
      "surpass IN text_03\n",
      "se agregó:  1.0 en:  2 101\n",
      "experts NOT IN text_01\n",
      "se agregó:  0.0 en:  0 102\n",
      "experts NOT IN text_02\n",
      "se agregó:  0.0 en:  1 102\n",
      "experts IN text_03\n",
      "se agregó:  1.0 en:  2 102\n",
      "several NOT IN text_01\n",
      "se agregó:  0.0 en:  0 103\n",
      "several NOT IN text_02\n",
      "se agregó:  0.0 en:  1 103\n",
      "several IN text_03\n",
      "se agregó:  1.0 en:  2 103\n",
      "tasks NOT IN text_01\n",
      "se agregó:  0.0 en:  0 104\n",
      "tasks NOT IN text_02\n",
      "se agregó:  0.0 en:  1 104\n",
      "tasks IN text_03\n",
      "se agregó:  1.0 en:  2 104\n",
      "trend NOT IN text_01\n",
      "se agregó:  0.0 en:  0 105\n",
      "trend NOT IN text_02\n",
      "se agregó:  0.0 en:  1 105\n",
      "trend IN text_03\n",
      "se agregó:  1.0 en:  2 105\n",
      "intensify NOT IN text_01\n",
      "se agregó:  0.0 en:  0 106\n",
      "intensify NOT IN text_02\n",
      "se agregó:  0.0 en:  1 106\n",
      "intensify IN text_03\n",
      "se agregó:  1.0 en:  2 106\n",
      "making NOT IN text_01\n",
      "se agregó:  0.0 en:  0 107\n",
      "making NOT IN text_02\n",
      "se agregó:  0.0 en:  1 107\n",
      "making IN text_03\n",
      "se agregó:  1.0 en:  2 107\n",
      "progress NOT IN text_01\n",
      "se agregó:  0.0 en:  0 108\n",
      "progress NOT IN text_02\n",
      "se agregó:  0.0 en:  1 108\n",
      "progress IN text_03\n",
      "se agregó:  1.0 en:  2 108\n",
      "computation NOT IN text_01\n",
      "se agregó:  0.0 en:  0 109\n",
      "computation NOT IN text_02\n",
      "se agregó:  0.0 en:  1 109\n",
      "computation IN text_03\n",
      "se agregó:  1.0 en:  2 109\n",
      "becomes NOT IN text_01\n",
      "se agregó:  0.0 en:  0 110\n",
      "becomes NOT IN text_02\n",
      "se agregó:  0.0 en:  1 110\n",
      "becomes IN text_03\n",
      "se agregó:  1.0 en:  2 110\n",
      "cheaper NOT IN text_01\n",
      "se agregó:  0.0 en:  0 111\n",
      "cheaper NOT IN text_02\n",
      "se agregó:  0.0 en:  1 111\n",
      "cheaper IN text_03\n",
      "se agregó:  1.0 en:  2 111\n",
      "therefore NOT IN text_01\n",
      "se agregó:  0.0 en:  0 112\n",
      "therefore NOT IN text_02\n",
      "se agregó:  0.0 en:  1 112\n",
      "therefore IN text_03\n",
      "se agregó:  1.0 en:  2 112\n",
      "clearly NOT IN text_01\n",
      "se agregó:  0.0 en:  0 113\n",
      "clearly NOT IN text_02\n",
      "se agregó:  0.0 en:  1 113\n",
      "clearly IN text_03\n",
      "se agregó:  1.0 en:  2 113\n",
      "one NOT IN text_01\n",
      "se agregó:  0.0 en:  0 114\n",
      "one NOT IN text_02\n",
      "se agregó:  0.0 en:  1 114\n",
      "one IN text_03\n",
      "se agregó:  1.0 en:  2 114\n",
      "topics NOT IN text_01\n",
      "se agregó:  0.0 en:  0 115\n",
      "topics NOT IN text_02\n",
      "se agregó:  0.0 en:  1 115\n",
      "topics IN text_03\n",
      "se agregó:  1.0 en:  2 115\n",
      "stay NOT IN text_01\n",
      "se agregó:  0.0 en:  0 116\n",
      "stay NOT IN text_02\n",
      "se agregó:  0.0 en:  1 116\n",
      "stay IN text_03\n",
      "se agregó:  1.0 en:  2 116\n",
      "great NOT IN text_01\n",
      "se agregó:  0.0 en:  0 117\n",
      "great NOT IN text_02\n",
      "se agregó:  0.0 en:  1 117\n",
      "great IN text_03\n",
      "se agregó:  1.0 en:  2 117\n",
      "time NOT IN text_01\n",
      "se agregó:  0.0 en:  0 118\n",
      "time NOT IN text_02\n",
      "se agregó:  0.0 en:  1 118\n",
      "time IN text_03\n",
      "se agregó:  1.0 en:  2 118\n",
      "get NOT IN text_01\n",
      "se agregó:  0.0 en:  0 119\n",
      "get NOT IN text_02\n",
      "se agregó:  0.0 en:  1 119\n",
      "get IN text_03\n",
      "se agregó:  1.0 en:  2 119\n",
      "involved NOT IN text_01\n",
      "se agregó:  0.0 en:  0 120\n",
      "involved NOT IN text_02\n",
      "se agregó:  0.0 en:  1 120\n",
      "involved IN text_03\n",
      "se agregó:  1.0 en:  2 120\n",
      "excellent NOT IN text_01\n",
      "se agregó:  0.0 en:  0 121\n",
      "excellent NOT IN text_02\n",
      "se agregó:  0.0 en:  1 121\n",
      "excellent IN text_03\n",
      "se agregó:  1.0 en:  2 121\n",
      "starting NOT IN text_01\n",
      "se agregó:  0.0 en:  0 122\n",
      "starting NOT IN text_02\n",
      "se agregó:  0.0 en:  1 122\n",
      "starting IN text_03\n",
      "se agregó:  1.0 en:  2 122\n",
      "point NOT IN text_01\n",
      "se agregó:  0.0 en:  0 123\n",
      "point NOT IN text_02\n",
      "se agregó:  0.0 en:  1 123\n",
      "point IN text_03\n",
      "se agregó:  1.0 en:  2 123\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "\n",
    "for word_termns in dicc_termns: #dicc_termns todos los términos\n",
    "#    print()\n",
    "    for word_texts in dicc_texts: #dicc_texts todos los textos\n",
    "#        print(\"EVALUAR:\", word_termns, \"EN: \", word_texts)\n",
    "        if(word_termns in dicc_texts[word_texts]): #si está\n",
    "            print(word_termns, \"IN\", word_texts)\n",
    "            \n",
    "            matrix[j, i] = dicc_termns[word_termns]\n",
    "            \n",
    "        elif(word_termns not in dicc_texts[word_texts]): # si no está\n",
    "            print(word_termns, \"NOT IN\", word_texts)\n",
    "            \n",
    "            matrix[j, i] = 0\n",
    "            \n",
    "            \n",
    "        print(\"se agregó: \", matrix[j,i], \"en: \", j, i)\n",
    "            \n",
    "        j = j + 1\n",
    "        \n",
    "    j = 0\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 9., 4., 4., 2., 2., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1.,\n",
       "        1., 2., 2., 1., 1., 4., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 4., 4., 0., 2., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0.,\n",
       "        0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 1., 1., 3., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        2., 2., 1., 1., 1., 1., 2., 1., 1., 1., 1., 2., 1., 2., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 4., 4., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        2., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 2., 0., 2., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 2.,\n",
       "        1., 1., 3., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 124)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3081824980742892"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cos_t01_t02 = dot(matrix[0],matrix[1])/(norm(matrix[0])*norm(matrix[1]))\n",
    "df_cos_t01_t02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5428817639036149"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cos_t02_t03 = dot(matrix[1],matrix[2])/(norm(matrix[1])*norm(matrix[2]))\n",
    "df_cos_t02_t03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24975922174901077"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cos_t01_t03 = dot(matrix[0],matrix[2])/(norm(matrix[0])*norm(matrix[2]))\n",
    "df_cos_t01_t03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcion(df, idioma1, idioma2):\n",
    "    contador = 0\n",
    "    \n",
    "    for documento in df:\n",
    "        text_tokens = tokenizer.tokenize(documento.lower()) \n",
    "\n",
    "        \n",
    "\n",
    "        for word in text_tokens:\n",
    "            temp = \"text_\" + tokens_wout_stopwords\n",
    "            if word not in stopwords.words(idioma1): \n",
    "                \n",
    "                temp = []\n",
    "                temp + _tokens_wout_stopwords.append(word)\n",
    "            elif word not in stopwords.words(idioma2): text_tokens_wout_stopwords.append(word)\n",
    "            \n",
    "            dicc_texts.update({: text_tokens_wout_stopwords})\n",
    "        \n",
    "        contador = contador + 1    \n",
    "    \n",
    "    return dicc_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer.tokenize(text_01.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text_tokens_01'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"text_tokens\" + \"_01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
